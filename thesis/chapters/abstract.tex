\begin{abstract}

Reinforcement Learning (RL) aims to learn agent behavioural policies by maximizing the cumulative reward obtained by interacting with the environment.
Standard RL approaches learn an end-to-end mapping from observations to action spaces which define the agent's behavior.
On the other hand, Foundational Models learn different representations of the world which can be used by agents to accelerate the learning process.
In this thesis, we study how to combine these representations to create an enhanced state representation.
Specifically, we propose a technique called Weight Sharing Attention (WSA) which combines embeddings of different Foundational Models, and we empirically assess its performance against alternative combination modules.
We tested our approach on different Atari games, and we analyzed the issue of out-of-distribution data and how to mitigate it.
We showed that, without fine-tuning of hyperparameters, WSA obtains comparable performance with state-of-the-art methods.
This method is effective and could allow life-long learning agents to adapt to different scenarios over time.



%via agent's interaction with a specific environment. Usually, the go-to approach to tackle this problem is to learn from scratch an end-to-end mapping between the observation and action spaces to define agents' behaviors.
%On the other hand, Foundational Models (FMs) can share insightful representations of the world ready to be leveraged by agents. Different FMs learn disparate latent embeddings, also coming from different modalities. How to effectively combine these representations is still an open and understudied question.
%In this work, we propose Weight Sharing Attention (WSA) to combine embeddings of different FMs to shape an enriched state representation. We compare several combination modes showing that, without fine-tuning of hyperparameters, WSA obtains comparable performance with state-of-the-art methods on different Atari games. Furthermore, we demonstrate and analyse one of the principal challenges and limitations of this approach: the issue of out of distribution data.



%In this work, we present a technique for deep reinforcement learning that, through the use of different state representations, is able to combine different features in order to create an enhanced feature rapresentation.
%Instead of directly learning a policy from raw input, the agent uses pre-trained foundational models which are capable of extracting features of various forms.

\end{abstract}
