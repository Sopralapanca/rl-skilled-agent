%! Author = giaco
%! Date = 16/05/2024

\chapter{Experiments \& Results}
\label{sec:experiments}

\subsection{Experimental Setup}\label{sec:exp-setup}
To simulate and interact with the Atari environments \citep{bellemare2013atari} we leverage the API provided by Gymnasium \citep{towers_gymnasium_2023}. The training performance was tracked and recorded using \texttt{Weights and Biases} \citep{wandb}. We build our work\footnote{Upon acceptance we will publicly release on GitHub the code to replicate all the experiments.} using \texttt{Stable-Baselines3} (SB3) \citep{stable-baselines3} as a backbone to create agents architecture and use reliable implementations of RL algorithms. In particular, we exploit the interface provided by SB3 that allows to decompose the agents' network in \texttt{Feature Extractor} and \texttt{Fully-Connected Network}, as also highlighted in Figure \ref{fig:main_architecture}. We use the \texttt{Feature Extractor} to define the different combination modules providing as output the composed latent representation. The \texttt{Fully-Connected Network} receives as input the FMs' unified linear encoding and maps it to actions (or values). We keep the latter component as simple as possible since most of the information should be provided by FMs' representations. Throughout all the experiments we use the hyperparameter provided by \citet{rl-zoo3} for different RL algorithms. We do not conduct any hyperparameter search for our agents. Instead, we only modify the model architecture. All the values are reported in Appendix \ref{sec:app-exp-setup}.

\subsection{FMs Data and Training}
To train the different FMs we first create a specific dataset for each game. For each environment, we collected \texttt{1M} frames via random agents interacting with the environment. The dataset contains grayscaled game frames with size 84x84 pixels. During the training process of the models, data is randomly sampled to avoid any correlation between elements due to the collection phase. For the main experiments of our approach, we use three different models which provides us four different pre-trained networks: \texttt{Video Object Segmentation} \citep{goel2018unsupervised}, \texttt{State Representation} \citep{anand2019unsupervised}, and \texttt{Object Keypoints} \citep{kulkarni2019unsupervised}. Additionally, we train a deep autoencoder - inspired by Nature CNN \citep{mnih2015human} - to encode the current state and leverage its representation to compute the context. We implement and train the models for all the games using the default architecture and hyperparameters, the values and additional details are reported in Appendix \ref{sec:app-models}. As previously mentioned, the main objective of this work is the \texttt{combination} of different representations, thus we exploit small models, which can be easily changed to more complex models without affecting the structure of our work. While training the agent, pre-trained models' weights are frozen and no longer updated.

\subsection{Feature Extractors}\label{sec:extractors}
Besides our proposed methodology, we tested and compared the performance of several combination modules. All solutions act as interchangable modules inside the \texttt{Feature Extractor}, i.e. only need to replace the \textcolor{orange}{yellow} component in Figure \ref{fig:main_architecture}. Their output is a linear representation that is provided as input to the \texttt{Fully-Connected Network}. We report a rundown across all solutions:
\begin{itemize}
    \item \texttt{Linear} (LIN): pre-trained models' representations are linearized and concatenated.
    \item \texttt{Fixed Linear} (FIX): embeddings are linearized to a predefined size, possibly using adapters to scale the information.
    \item \texttt{Convolutional} (CNN): FMs' outputs are concatenated along the channel dimension. They are processed by a predefined number of convolutional layers and the resulting information is flattened to linear.
    \item \texttt{Mixed} (MIX): this is a combination of the previous methods. Data coming from different spaces are dealt separately and then combined.
    \item \texttt{Reservoir} (RES): inspired by \citet{gallicchio2017}, this approach leverages reservoir layers to combine models' representations.
    \item \texttt{Dot Product Attention} (DPA): similarly to \texttt{Fixed Linear} the representations are reduced to a specific size - key and value. Via \texttt{scaled dot product attention} \citep{vaswani2017attention} we compute the final weighted representation using as query vector the representation obtained from the State Encoder.
\end{itemize}

\subsection{Initial Experiments}\label{sec:init_exp}
We chose three different Atari games as benchmark for our work: \texttt{Pong}, \texttt{Ms.Pacman}, and \texttt{Breakout}. After training each FMs, we run a first round of experiments for three games to find the three best performing extractors that will be used for our empirical analysis along with our proposed method. Agents are trained for \texttt{10M} steps using the parameters provided by \texttt{rl-zoo} and we use as RL algorithm PPO \citep{schulman2017proximal}. The \texttt{Fully-Connected Network} is set to a single layer of 256 units. Throughout the learning process, agents are repeatedly evaluated for 100 episodes. Appendix \ref{sec:app-com-mod} reports all the possible configurations and their performance in this experimental phase. We report the best performing modules for each game, the chosen embedding size is detailed between parentheses:
\begin{itemize}
    \item \texttt{Pong}: WSA (1024), RES (1024), CNN (2)
    \item \texttt{Ms.Pacman}: WSA (256), RES (1024), CNN (2)
    \item \texttt{Breakout}: WSA (256), FIX (512), CNN (3)
\end{itemize}

\subsection{Main Results}\label{sec:results_1}
In the following empirical evaluation, we used the same methodology and setup adopted for initial experiments. For each game, multiple agents instances are trained using different seeds. For a comprehensive view of agents' performance, the behavior is averaged over different versions. Figure \ref{fig:trainresults} shows the average learning curves of agents during training - the shaded area depicts the standard deviation. To ensure robust evaluation, agents are tested across \texttt{5 random seeds} for \texttt{50 episodes}, specifically: 47695, 32558, 94088, 71782 and 66638. Table \ref{tab:results} reports the averaged results during evaluation of the best agent during training. We address the elephant in the room - Figure \ref{fig:breakouttrain} - in the next section \ref{sec:breakout_study}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pong_train.png}
        \caption{\texttt{Pong}}
        \label{fig:pongtraining}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mspacman_train.png}
        \caption{\texttt{Ms.Pacman}}
        \label{fig:mspacmantrain}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/breakout_train.png}
        \caption{\texttt{Breakout}}
        \label{fig:breakouttrain}
    \end{subfigure}
    \caption{Cumulative reward during \texttt{training} of different agents, including WSA, PPO, and other combination modules, on three different Atari games. Each subfigure shows the mean score, with shaded areas indicating the standard deviations, across multiple agents.}
    \label{fig:trainresults}
\end{figure}

Focusing on the first two games, Figures \ref{fig:pongtraining}-\ref{fig:mspacmantrain}, we can see that our approach WSA and other combination modules yield a high reward in the first stages of the training process. This suggests that FMs provide an insightful and comprehensive base of knowledge straight out of the box.
It is important to remember that the results are obtained without conducting any hyperparameter search. Eventually end-to-end PPO catches up and reaches a higher final reward during training. Nevertheless, looking at the evaluation results in Table \ref{tab:results}, WSA matches the maximum reward (\texttt{21}) on \texttt{Pong} and achieves higher score (\texttt{2530}) on \texttt{Ms.Pacman} than PPO (\texttt{2258}). From these results, we can make two important considerations. The first one is the difference between training and evaluation scores. In particular, in \texttt{Ms.Pacman} is more marked, WSA provides a solid generalization for the task, scoring around \texttt{1250} during training compared to \texttt{2530} during evaluation. The second one concerns the difference compared to PPO final score during learning. This behavior is probably related to the well-known phenomenon of underfitting. In fact, with respect to end-to-end solutions, our approach has a limited number of components that are updated during training, in particular the \texttt{Fully-Connected Network} is only one layer. As sanity-check, to ensure agents' performance do not depend on the particular RL algorithm, we also compare WSA effectiveness on \texttt{Ms.Pacman} and \texttt{Breakout} using \texttt{DQN}. Training curves and evaluation scores are reported in Appendix \ref{sec:app-add-exp}.

\subsubsection{Breakout: Out of Distribution Data}\label{sec:breakout_study}
Unexpectedly on \texttt{Breakout}, WSA did not work straightaway. Both in Figure \ref{fig:breakouttrain} and in Table \ref{tab:results}, the performance of WSA is extremely low compared to PPO (\texttt{99 vs 413}).

A first attempt to overcome the problem was to increase the number of parameters of the model, adding more expressive power to the network learning the policy. We increased the size of the \texttt{Fully-Connected Network} to three linear layers of size 1024, 512, and 256 respectively and use ReLU as activation function. Figure \ref{fig:breakout_policy} and Table \ref{tab:results} - referenced as \texttt{(P)} - report the results for this experiment. With respect to the default scenario, there is an improvement in performance, but WSA is still far from PPO (\texttt{156 vs 413)}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/breakout_policy.png}
        \caption{Increasing MLP size}
        \label{fig:breakout_policy}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/breakout_expert.png}
        \caption{Using Mixed Data}
        \label{fig:breakout_expert}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/breakout_policy_mixed.png}
        \caption{Mixed Data + Increased MLP}
        \label{fig:breakout_expert_policy}
    \end{subfigure}
    \caption{Performance comparison of WSA with PPO across various strategies on \texttt{Breakout}. Each subfigure displays the average score with the standard deviation shaded. They report the different experiments to improve the performance of WSA: (\ref{fig:breakout_policy}) increasing the size of the \texttt{Fully-Connected Network} on performance, (\ref{fig:breakout_expert}) pre-training the models using random data and expert data, and (\ref{fig:breakout_expert_policy}) combining the effect of increased \texttt{Fully-Connected Network} and expert data during pre-training.}
    \label{fig:breakout_study}
\end{figure}

Before moving on to the next set of experiments, which close the gap between PPO and WSA, we address a key challenge when using pre-trained models in RL. Unlike \texttt{Pong} and \texttt{Ms.Pacman}, where the game screen remains relatively stable, \texttt{Breakout} dynamically evolves as more blocks are removed with score progression. This poses a \textbf{distributional shift} problem between training and test data. Our initial training data, collected from random agents, lacks late game scenarios with few blocks remaining, leading to limitations in FMs performance during evaluation. To tackle the problem and show the consequences of a limited training dataset, we gather new datasets from both random and expert agents to encompass early and late game scenarios.

We retrain all the FMs and RL agents - using a single layer \texttt{Fully-Connected Network}. Significant improvements are observed, as shown in Figure \ref{fig:breakout_expert} and Table \ref{tab:results}, using (\texttt{M}). Notably, WSA demonstrates a substantial increase in the agent's final score (\texttt{156 $\rightarrow$ 345}), approaching PPO performance, and echoing the trends analyzed in Section \ref{sec:results_1} for the other two games. To complete the set of experiments, we also test the combination of the previous configuration. Increased \texttt{Fully-Connected Network} and Mixed datasets (Figure \ref{fig:breakout_expert_policy}, \texttt{PM}) yields the best performing configuration of WSA with slightly lower but competitive score to PPO (\texttt{387 vs 413}). Additionally, Appendix \ref{sec:app-add-exp} reports the same analysis for the other combination modules presented in Figure \ref{fig:breakouttrain}.

Lastly, Figure \ref{fig:inter} illustrates the enhanced \texttt{explainability} provided by WSA. By examining the current frames and the corresponding weights allocated by the shared component to each pre-trained model, one can appreciate agents' decision-making process. The visualization reveals how various FMs are leveraged in distinct contexts, shedding light on agents' dynamic adaptation of its prior knowledge.

\begin{table}[ht]
\begin{minipage}[b]{0.49\linewidth}
\centering
    \begin{tabular}[b]{lll}
                \multicolumn{1}{l}{Environment}  &\multicolumn{1}{l}{\bf Agent} &\multicolumn{1}{l}{\bf Reward} \\
                \hline \\
                \multirow{5}{*}{\texttt{Pong}} & \textbf{CNN} & \textbf{21 $\pm$ 0.00} \\
                                      & RES & 20.85 $\pm$ 0.29 \\
                                      & \textbf{WSA} & \textbf{21 $\pm$ 0.00} \\
                                      & \textbf{PPO} & \textbf{21 $\pm$ 0.00}\\

                                      \hline \\
                \multirow{5}{*}{\texttt{Ms.Pacman}} & CNN & 1801.30 $\pm$ 20.95 \\
                                      & RES & 1369.27 $\pm$ 565.23 \\
                                      &\textbf{WSA} & \textbf{2530.20 $\pm$ 23.09} \\
                                      & \underline{PPO} & \underline{2258.40 $\pm$ 1.42}\\
                                      \hline \\

                \multirow{7}{*}{\texttt{Breakout}}
                                      & CNN & 65.98 $\pm$ 1.62 \\
                                      & FIX & 87.17 $\pm$ 6.87 \\
                                      & WSA & 99.58 $\pm$ 6.66 \\
                                      & WSA (P) & 156.17 $\pm$ 3.59 \\
                                      & WSA (M) & 345.52 $\pm$ 6.47 \\
                                      & \underline{WSA} (PM)& \underline{387.15 $\pm$ 0.43} \\
                                      & \textbf{PPO} & \textbf{413.51 $\pm$ 1.10}\\
    \end{tabular}
    \caption{Performance during \texttt{evaluation} averaged across 5 different seeds.}
    \label{tab:results}
\end{minipage}\hfill
\begin{minipage}[b]{0.49\linewidth}
\centering
\includegraphics[width=\textwidth]{images/744.png}
\includegraphics[width=\textwidth]{images/1524.png}
\captionof{figure}{Analyzing WSA explainability across different frames, showcasing the weights assigned to different FMs.}
\label{fig:inter}
\end{minipage}
\end{table}



% The architecture of Nature CNN has been slightly modified as it appears in Tab. \ref{tab:nature_cnn} to match the output dimensions of the other skills so that they can be properly concatenated.


%spiegare un po meglio questa parte, non abbiamo un modello generic ma alleniamo una singola skill per ogni ambient al contrario di SIMA
% Finally, all of these models are not general. We decided to specialize the skills by creating a model for each skill and environment.

% \begin{table}[htbp]
%     \begin{center}
%         \begin{tabular}{lllll}
%             \multicolumn{1}{l}{LAYER}  &\multicolumn{1}{l}{\bf IN. CHANNELS}  &\multicolumn{1}{l}{\bf OUT CHANNELS}  &\multicolumn{1}{l}{\bf KERNEL SIZE}  &\multicolumn{1}{l}{\bf STRIDE}
%             \\ \hline \\
%             1st CNN Layer   &  1 or 4  & 32 & 8 & 4 \\
%             2nd CNN Layer   &  32  & 64 & 3 & 1 \\
%             3rd CNN Layer   &  64  & 64 & 3 & 1 \\

%         \end{tabular}
%     \end{center}
%     \caption{This table shows the encoder architecture of Autoencoder, Image Completion, and Frame Prediction models. Input channels for the first convolutional layer are 1 or 4 depending on the model. Autoencoder and Image Completion take as input 1 frame in grayscale while Frame Prediction takes as input the last 4 grayscale frames stacked along the channel dimension. The stride on the second convolutional layer was decreased from 2 to 1 with respect to Nature CNN to match the output dimensions of other skills. Each convolutional layer is followed by a ReLU activation function. The decoder part is specular.}
%     \label{tab:nature_cnn}
% \end{table}

Regarding the experiments, we mainly used the Stable Baselines suite \cite{stable-baselines} and Gymnasium \cite{towers_gymnasium_2023} for our tests.
We decided to restrict our tests to only four environments, namely \textit{Pong, Breakout, Asteroids, and Ms. Pacman,} this was to avoid adding complexity and growing the time for experiments. We believe that this choice of games provides a fair compromise between simple games like Pong and much more complex games with lots of moving objects like Asteroids.
Furthermore, the choice of games was dictated by the availability of the RAM annotations provided in \cite{anand2019unsupervised}.
For each environment, we selected the \textit{NoFrameskip-v4} version of the game.

Our purpose was to change the representation learning part of PPO considering different skills to use and different ways of concatenating them, leaving the policy learning part as it is.

So, first of all, we needed a dataset on which to train the skills. For each environment, we then collected a dataset consisting of INSERIRE episodes using an agent playing randomly and preprocessing the game frames in the same way the agent would see them, thus in grayscale and of size 84x84 pixels.

Then, we defined a set of basic skills that are informative enough, so we chose the skills of Video Object Segmentation, Object Keypoints, and State Representation.
We implemented the three models as specified in their respective papers and used this newly created dataset to train the networks.
We then froze the weights of these skills and used them when training the agent.

A first round of experiments was conducted with the methods and parameters shown in Tab \ref{tab:firstround}.
Here we test all the concatenation methods shown in Sec. \ref{sec:extractors}, in this way we start to see which extractor works well for which game.
\begin{table}[htbp]
    \begin{center}
        \begin{tabular}{ll}
            \multicolumn{1}{l}{EXTRACTORS}  &\multicolumn{1}{l}{\bf PARAMETERS}
            \\ \hline \\
            Linear Concatenation              &  / \\
            Fixed Linear Concatenation        & 256, 512, 1024 \\
            CNN Concatenation                 & 1, 2, 3 \\
            Combine                           & / \\
            Reservoir Concatenation           & 512, 1024, 2048 \\
            Dot Product Attention             & 256, 512, 1024 \\
            Weights Sharing Attention         & 256, 512, 1024 \\

        \end{tabular}
    \end{center}
    \caption{This table shows the different configurations used for the extractors. For Fixed Linear Concatenation, Dot Product Attention, and Weights Sharing Attention the values indicate the fixed dimensions of skill embeddings and context. For Reservoir Concatenation, the values indicate the size of the reservoir. For CNN Concatenation the values indicate the number of convolutional layers.}
    \label{tab:firstround}
\end{table}

In the subsequent phase, the research progressed with a refined focus. We considered only the \textbf{three most effective} extractors identified from the initial experiments, and, if an extractor is already in the top 3, we consider the one immediately following with the best reward.
This strategic approach allowed for a deeper exploration of the performance nuances among the top contenders.
We provide a summary of the best extractors we chose for each game in Tab. \ref{tab:top3}.
As can be seen from the table at this stage, the best 3 agents for Breakout are missing because the agents failed to be competitive with a PPO agent and require a more in-depth study which will be done later.
For the execution of these experiments, a seed was set for reproducibility. We used the Atari Wrapper for each environment to preprocess the observations as standard procedure. Agents are trained for \textit{10.000.000} steps using the parameters provided by Stable Baselines in rl-zoo Github repository for Atari games considering PPO \cite{rl-zoo3}.
For each \textit{40,000} steps agents were evaluated for 100 episodes.

\begin{table}[htbp]
    \begin{center}
        \begin{tabular}{ll}
            \multicolumn{1}{l}{\bf GAME}  &\multicolumn{1}{l}{\bf EXTRACTORS}
            \\ \hline \\
            Pong       & Weights Sharing (1024), Reservoir (1024), CNN Concat. (2) \\
            Ms. Pacman & Weights Sharing (256), Reservoir (1024), CNN Concat. (2) \\
            Asteroids         & Reservoir (512), Reservoir (1024) \\
            Breakout          & Weights Sharing (256), Fixed Lin. Concat. (512), CNN Concat (3) \\
        \end{tabular}
    \end{center}
    \caption{In the table we see the extractors chosen for each game. The number in parentheses indicates the fixed embedding size for skills or context for the weights sharing extractor, the size of the reservoir for the reservoir extractor, and the number of convolutional layers for the CNN extractor.}
    \label{tab:top3}
\end{table}

Fig. \ref{fig:trainresults}, instead shows the learning curves of agents during training, the curves are smoothed using a running average.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/pong_train.png}
        \caption{Pong}
        \label{fig:pongtraining}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/breakout_train.png}
        \caption{Breakout}
        \label{fig:asteroidstrain}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mspacman_train.png}
        \caption{Ms. Pacman}
        \label{fig:mspacmantrain}
    \end{subfigure}

    \caption{write caption}
    \label{fig:trainresults}
\end{figure}


As can be seen from the graphs of the learning curves, in some cases such as Pong and Ms. Pacman, the various skilled agents manage to learn faster than standard PPO. Meanwhile, in Asteroids, the learning speed is comparable with PPO.
The chosen skills track moving objects and are very useful in games such as Pong whose underlying environment does not change as the game progresses, this allows the agent to learn faster by concentrating only on the moving ball and moving bars instead of the whole game frame.

%analisi su breakout
Regarding Breakout, an initial analysis of the experiments performed shows that skill-equipped agents are not competitive with a PPO agent. This could be for various reasons, such as the fact that the skills are only trained on frames where the agent plays randomly, and as Breakout is a game where the structure changes as it progresses when the bricks are destroyed, the skills may no longer be informative in the middle or final stages of the game and even confuse the agent.
Another reason could be the fact that the skills used so far mostly track moving objects, which in Breakout are the ball and the bar. The agent may therefore have little or no information about the bricks, which are a static part of the frame.
One more reason could be that the policy learning part of the algorithm was untouched and bigger networks may extract more information from the skills.

To this end, we have made other experiments by first collecting a dataset of 1000 episodes, 500 of which are frames of an agent that plays random while the other 500 are collected using a trained PPO agent. In this way, we added variety to the dataset and we trained again the skills using this new data. We refer to these skills as \textbf{expert skills}.
As extractors, we restricted the tests only to the use of Weights Sharing with sizes 1024 and 256, as these are the ones that performed best in the other experiments, we then chose for comparison Reservoir extractor with size 1024 and Combine. We also decided to focus only on a subset of extractors to decrease the testing time.
The results of this experiment can be seen in Fig. \ref{fig:breakout_expert}

Next, we ran another experiment using expert skills and also increasing the policy network dimension of the algorithm. Standard PPO uses a linear layer of size 256 both for the policy network and value function network, we instead increased this network using three linear layers of size 1024, 512, and 256 respectively for the first, second, and third one. We use ReLU as activation function after each linear layer to add non-linearity.
The results are shown in Fig \ref{fig:breakout_expert_and_policy}.

Finally, the last round of experiments was conducted by including the skills of Image Completion and Frame Prediction in addition to the three previously used. The skills are all used in expert mode and we also increased the policy learning network as before.
For this last experiment, we used only the agent that performed best in the other experiments, namely Weights sharing with embedding dimensions 256 and 1024.
Fig. \ref{fig:breakout_expert_policy_skills} shows the results.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/breakout_policy.png}
        \caption{Breakout con MLP + quello che faceva schifo}
        \label{fig:breakout_expert}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/breakout_expert.png}
        \caption{Breakout stesso di prima ma con dati expert}
        \label{fig:breakout_expert_and_policy}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/breakout_p+m.png}
        \caption{Breakout expert  + policy}
        \label{fig:breakout_expert_policy_skills}
    \end{subfigure}
    \hfill

    \caption{PPO
 rimane uguale in tutti e 3}
    \label{fig:trainresults}
\end{figure}



Tab. \ref{tab:results} shows the highest reward obtained for each game by the top three agents of each game considering the evaluation. This table also presents the agent called PPO which represents our training run of a standard PPO agent without skills, and the agent REFERENCE which is the result reported on the Hugging Face page of Stable-Baselines.

\begin{table}[htbp]
    \begin{center}
        \begin{tabular}{llll}
            \multicolumn{1}{l}{AGENT}  &\multicolumn{1}{l}{\bf PONG} &\multicolumn{1}{l}{\bf BREAKOUT} &\multicolumn{1}{l}{\bf MS. PACMAN}
            \\ \hline \\
            Weights Sharing (1024) &  20 $\pm$ 00 &  - &  -  \\
            Weights Sharing (256)  &  - &  356 $\pm$ 00 &  2487 $\pm$ 00  \\
            Reservoir (1024)       &  20 $\pm$ 00 &  - &  2111 $\pm$ 00 \\
            CNN (2)                &  20 $\pm$ 00 &  - &  1812 $\pm$ 00 \\
            CNN (3)                &  - &  50 $\pm$ 00 &  - \\
            Fixed Lin. (512)                &  - &  50 $\pm$ 00 &  - \\
            PPO                    &  20 $\pm$ 00 &  387 $\pm$ 00 &  2230 $\pm$ 00 \\
            REFERENCE              &  21 $\pm$ 0 &  398 $\pm$ 16.30 &  1659 $\pm$ 144.81 \\
        \end{tabular}
    \end{center}
    \caption{Tra parentesi le dimensioni}
    \label{tab:results}
\end{table}

%As can be seen in \ref{fig:breakout_expert} we tested Breakout with expert skills only on a subset of a few extractors. We chose Weights Sharing Attention with 256 and 1024 as dimensions for skills embeddings, Combine Extractor, and Reservoir Extractor with 1024 as dimension of the reservoir. We restricted only to these extractors to increase the time for experiments too much.
%We can see that only by using expert skills the Weights Sharing agent become comparable with the PPO agent, it learns more slowly but eventually in evaluation manages to have a score very similar to PPO. Reaching xxx while PPO reaches xxx.

%Let us now analyze the use of expert skills and the increase of the network involved in policy learning.
%We can see in Fig. \ref{fig:breakout_expert_and_policy} that Weights Sharing Attention with dimension 256 for skills embeddings is the one that performs the best. It is comparable with a PPO agent since they perform basically the same.

%Finally, for what regards the use of more skills in Fig. \ref{fig:breakout_expert_policy_skills} we can see ...


%analisi weights sharing
%In most of the experiments we have performed, we have noticed that weights sharing attention as a way of concatenating different skill embeddings is the one that performs best.
%It, being more general as a method manages to better filter out noisy input, focusing only on the information that is really important for the agent to learn.
INSERIRE ANALISI WEIGHTS SHARING
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum

INSERIRE ANALISI SUL NUMERO DI PARAMETRI TOTALI DI PPO E SKILLED AGENT
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum

INSERIRE ANALISI SUGLI FPS TRA PPO E SKILLED AGENT
Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum

