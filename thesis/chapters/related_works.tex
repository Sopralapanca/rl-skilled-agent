%! Author = giaco
%! Date = 16/05/2024

\chapter{Related Work}
\label{sec:related_work}
The concept of helping an RL agent by providing information already processed by other models rather than learning from scratch to extract important features is not new.
Most of the works apply a single model that pre-processes the input and then learns the actual policy.
\cite{goel2018unsupervised} proposed an unsupervised technique for deep reinforcement learning that automatically detects moving objects given recent frames of Atari games. The learned representation is used instead of the raw frame to focus the policy of the agent on the moving objects.
The work of \cite{kulkarni2019unsupervised} provided an unsupervised method to learn object representations in terms of keypoints given video frames. They then used keypoints as inputs to policies instead of RGB observations.
\cite{anand2019unsupervised} introduced a method that learns state representations on Atari games. ESTENDERE UN PO.
\cite{montalvo2023exploiting} used a semantic segmentation model to transform Super Mario Bros frames from their original domain to a simplified semantic domain, and then trained the reinforcement learning algorithm with these simplified images.
All these methods process the input in one way and do not consider combining several methods that extract different features.

To the best of our knowledge, the first attempt to combine different types of features extracted from different models in RL is in \cite{raad2024scaling} where they provided a framework called SIMA that combines information coming from text encoding, image encoding, and video prediction encoding using transformers architectures to select keyboard and mouse actions in a variety of 3D games. \cite{obando2024mixtures} instead combines different embeddings using a mixture of experts approach.

% ADD
% https://proceedings.neurips.cc/paper_files/paper/2022/hash/548a482d4496ce109cddfbeae5defa7d-Abstract-Conference.html
% https://arxiv.org/abs/2107.03380
