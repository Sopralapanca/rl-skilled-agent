%! Author = giaco
%! Date = 16/05/2024

\chapter{Introduction}
\label{sec:introduction}
Reinforcement Learning (RL) \citep{sutton1998introduction} is one of the basic paradigms of Machine Learning (ML). It consists of having an agent learn the optimal behavior in a dynamic environment to maximize the cumulative reward.
In particular, RL mimics how we, as humans, learn.
It formulates the learning process as a sequence of interactions with the environment and RL algorithms use a reward-and-punishment paradigm as they process data.
Agents learn to solve a specific task based on their experiences with the world, they learn from the feedback of each action and self-discover the best processing paths to achieve final outcomes.

Over the past decades, RL algorithms have increasingly improved, achieving amazing results in many tasks and being adopted in new fields.
For example, RL can be applied in many real-world scenarios some of them are applications like recommendation systems where RL can customize suggestions to individual users based on their interactions, or for financial predictions where RL algorithms can optimize long-term returns by considering transaction costs and adapting to market shifts, and finally, it can be used in industry automation to teach robots to act more efficiently than humans or for self-driving cars and many other applications.
To name a few important works that are noteworthy throughout the history of RL \citet{watkins1992q} introduced \textbf{Q-Learning}, which is an RL algorithm to learn the value of an action in a particular state and has become one of the most well-known and widely used RL algorithms.
\citep{sutton1999policy} introduced policy gradient methods that are very useful, especially in scenarios involving continuous action spaces.

Specifically, in terms of work more on topic with the purpose of this thesis,
Starting from the first pioneering works of \citet{mnih2013playing, mnih2015human}, which started to explore the use of deep neural networks in games - such as Arcade Learning Environment (ALE) \citep{bellemare2013atari} -, a completely different perspective and scenario has been unleashed. As a matter of fact, an increasing interest has been gathering towards Deep Reinforcement Learning (DRL). DRL has produced substantial advancements in various research fields and applications, such as games, beating all Atari games at super-human performance with a single algorithm \citep{agent57} or defeating pro players in games like GO, StarCraft II and DOTA \citep{alphago, starcraft, dota}, but also in robotics \citep{rlrob, bousmalis2023robocat} and lastly toward general agents for 3D environments including different data modalities \citep{sima2024}.

In a typical RL setting, agents receive as input the raw representation of the state, without any additional information on the elements that characterize it. The learning process, which is focused on creating a policy - a mapping from states to actions which determines agent behaviour - implicitly hides a significant effort related to understanding how to process the input representations. While one of the promises of deep learning algorithms is to automatically construct well-tuned features, such representation might not emerge from end-to-end training of deep RL agents. Agents learn how to solve the task while indirectly learning how to process and extract useful information from the input features. While this approach has been the solution for several works, it adds an additional layer of complexity over RL algorithms eventually requiring significant computational resources.

Several works analyze the differences between the learning process of humans and RL agents, highlighting how prior knowledge can influence the learning curve \citep{lake2017building, dubey2018investigating}. Humans and agents define the learning pattern by incrementally improving their abilities. Human learning often involves incremental skill development, leveraging prior knowledge, and adapting strategies based on experience.
% Throughout the course of their lives, humans acquire skills and can immediately recognize objects in an image, distinguish elements in the foreground from those in the background, read, or even predict future evolution of the environment.
In new environments with few interactions, humans can leverage their prior abilities to understand the effects of their actions. This allows them to focus mostly on learning a good policy, rather than also learning how to interpret the environment. How can we encode and represent abilities in RL agents?

In light of this question, Foundational Models (FMs) present themselves as good candidates. They are trained on extensive datasets in a self-supervised fashion, enabling them to learn rich and generalizable representations. These representations can be leveraged in RL to accelerate the learning process and improve performance on complex tasks. Leveraging FMs' knowledge, RL agents can achieve higher efficiency and effectiveness, even in environments with sparse rewards or limited data. The objective of this paper is to study how different FMs' embeddings can be \texttt{combined} to compute a rich and informative representation of the environment. On that note, we will use relatively small models, but all the work can be scaled without additional efforts.
In this way, we can focus on \texttt{how} RL agents can exploit prior knowledge provided by FMs and focus the learning process on the actual policy. Here we report the main contributions of this work:

\begin{itemize}
    \item We propose Weight Sharing Attention (WSA) as combination module to incorporate different encodings coming from several FMs. This approach achieves comparable performance with end-to-end solutions, while also adding scalability and explainability to RL agents.
    \item We run experiments across 3 different Atari games (\texttt{Pong}, \texttt{MsPacman} and \texttt{Breakout}) studying the behavior of 7 different combination modules. To the best of our knowledge, this is the first analysis focused on combination of FMs' latent representation in RL.
    \item We show that without any fine tuning of hyperparameters our methodology can achieve competitive results with established RL end-to-end solutions. Moreover, we prove and address the problem of out of distribution data, which presents itself as one of the main challenges for these approaches. 
\end{itemize}




In recent years, Reinforcement Learning (RL) \cite{sutton1998introduction} algorithms have improved more and more, achieving amazing results in many tasks.
Recent studies, made significant progress in various research areas like competitive and strategic games \cite{mnih2013playing}, \cite{badia2020agent57}, \cite{vinyals2019grandmaster}, robotics \cite{bousmalis2023robocat} and chemistry \cite{varadi2022alphafold}.
Most of this work has benefited from developments in Deep Learning (DL) techniques, thus creating the Deep Reinforcement Learning paradigm (DRL) \cite{zhang2020deep}.
In particular, many methods manage to outperform human players in the majority of Atari games in the Arcade Learning Environment \cite{bellemare13arcade}.
These methods work on images, particularly game frames, and despite their effectiveness, they consist of training a model from scratch every time to solve one specific task and therefore require a lot of interaction with the environment to perform well, much more than a human.
Usually in the RL setting, agents are fed as input the raw representation of the state, without any additional information on the elements that characterize it, nor meaningful representation. In the learning process, agents, given the input, learn how to solve the task while indirectly learning how to compute a representation of the input features. These representations though, are \textit{reward-oriented}, hence they are not general and robust, but they are overfitted to be optimal for the current task. This leads to weak representation that as soon the task objective changes, it becomes sub-optimal and agents are not able to easily adapt. So the better the representation of the state the better the agent will perform.
On the other hand, humans, in the course of their lives, acquire skills and can immediately recognize important objects in an image, distinguish objects in the foreground from those in the background, read, or even predict the next frame after seeing the last moments.
Using these prior skills, when they are in the presence of new environments, humans, through little interaction with it, can understand what effects their actions will have and thus can focus more on learning a good policy to solve the task rather than learning about the environment.
Following this idea, we propose an approach where the RL agent is equipped with a set of skills in the form of pre-trained computer vision models. These models are capable of extracting different information from the game frames.
The aim of this study is, therefore, to understand which skills to equip the agent with, and how to combine these skills together to help the agent focus on the policy learning part rather than the representation learning part.
We tested our methods against an agent trained via the Proximal Policy Optimization (PPO) algorithm \cite{schulman2017proximal}.


%mancano due parole sui risultati ottenuti
INSERIRE QUALCHE FRASE SUI RISULTATI OTTENUTI

The work is organized in this way: Sec. \ref{sec:related_work} provides information about related works on equipping an agent with skills. Sec. \ref{sec:method} shows information about our approach. Sec. \ref{sec:experiments} provides the experiments that we conducted to test our method. Sec. \ref{sec:results} shows the results and finally Sec. \ref{sec:conclusions} provides the conclusions of the work and discusses possible future extensions.
