%! Author = giaco
%! Date = 16/05/2024

\chapter{Introduction}
\label{ch:introduction}
Reinforcement Learning~\citep{sutton1998introduction} is one of the paradigms of Machine Learning (ML).
It consists of an agent learn the optimal behavior in an environment that can change over time.
Its objective is to maximize the cumulative reward that it obtained as a result for a sequence of actions.
In particular, RL mimics how we, as humans, learn.
It formulates the learning process as a sequence of interactions with the environment and uses a trial-and-error approach to process data.
Agents learn to solve a specific task based on their experiences with the world, they learn from the feedback of each action, i.e.\ reward,  and discover the best way to achieve their goal.

Over the past decades, RL algorithms have increasingly improved, achieving amazing results in many tasks and being adopted in new fields~\citep{li2017deep, zhang2020deep}.
For example, RL can be applied in many real-world scenarios, some of them are applications like recommendation systems~\citep{zhao2019deep} where RL agents can customize suggestions to individual users based on their interactions, or for financial predictions~\citep{zhang2019deep} where RL algorithms can optimize long-term returns by considering transaction costs and adapting to market shifts.
Finally, it can be used in industry automation~\citep{levine2018learning} to teach robots to act more efficiently than humans or for self-driving cars~\citep{kiran2021deep} and many other applications.
Some recent achievements in RL that are related to the topics that will be developed in this thesis are
the works of \citet{mnih2013playing, mnih2015human}, which started to explore the use of deep neural networks in games, specifically using the Arcade Learning Environment (ALE)~\citep{bellemare2013atari}.
As a result of these works, although based on problems that may seem as basic as those in video games, new development perspectives have been created and an increasing interest has turned toward Deep Reinforcement Learning (DRL).
In fact, DRL has produced substantial advancements in various research fields and applications, such as games as already mentioned, beating all Atari games at super-human performance with a single algorithm~\citep{agent57} or defeating pro players in games like GO, StarCraft II and DOTA~\citep{alphago, starcraft, dota}, but also in robotics~\citep{rlrob, bousmalis2023robocat} and lastly toward general agents for 3D environments including different data modalities~\citep{sima2024}.

Discussing some of the problems highlighted in this thesis concerning the current algorithms in the literature, in a typical RL setting, agents receive as input state, without any additional information on the elements that characterize it.
Learning consists of creating a map from input to action space i.e.\ creating a policy.
A DRL agent trains a model from scratch to solve one specific task and, therefore, requires a lot of interaction with the environment to perform well, much more than the number of actions a human would need.
End-to-end solutions for the learning process implicitly hide a significant effort related to understanding how to process the input representations in the best possible way.
While one of the promises of deep learning algorithms is to automatically construct well-tuned representations, the same might not emerge from the training of deep RL agents. 
This is because agents learn how to solve the task while indirectly learning how to process and extract useful information from the input features. 
While this approach has been the solution for several works, it adds a layer of complexity to RL algorithms eventually requiring significant computational resources.

Several works analyze the differences between the learning process of humans and RL agents, highlighting how prior knowledge can influence the learning curve~\citep{lake2017building, dubey2018investigating}.
Agents, in the same way as humans, can define the learning pattern by incrementally improving their abilities.
Learning for humans, however, consists of acquiring new skills and incrementally improving those already known, leveraging prior knowledge, and adapting strategies based on experience.
In new environments with few interactions, humans can make use of their prior abilities to accomplish basic tasks like distinguishing new elements or reading guidelines.
Humans can understand the effects of their actions and this allows them to focus mostly on learning a good policy, rather than also learning how to interpret the environment. 
The idea behind this thesis is, therefore, to provide RL agents with abilities, just like humans, and aims to study a way to encode skills in such a way to incorporate prior knowledge into an RL agent.
In this way, we can focus on how RL agents can exploit prior knowledge provided their abilities and focus the learning process on the actual policy.


In the development of this thesis, we will then try to answer two questions:
\begin{itemize}
    \item How can we represent skills for an RL agent?
    \item Once skills are defined, how can we encode them and combine information coming from different skills or choose which one to use?
\end{itemize}


The thesis is organized in this way: Ch.\ref{ch:background} will introduce and explain all the techniques that are adopted in this work.
Ch. \ref{ch:related_work} provides information about related works and current state-of-the-art.
Ch. \ref{ch:method} shows information about our approach.
Ch. \ref{ch:experiments_and_results} provides the experiments that we conducted to test our method along with the results for each one, and finally Ch. \ref{ch:conclusions} provides the conclusions of the work and discusses possible future extensions.
